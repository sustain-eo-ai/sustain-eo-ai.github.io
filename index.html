<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SEA – ICCV 2025 Workshop</title>
  <style>
    /* :root { --accent: #10b981; } */
    :root { --accent: #0ABAB5; }
    * { box-sizing: border-box; scroll-behavior: smooth; }
    body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
      line-height: 1.6;
      color: #111;
      background: #f8fafc;
    }

    /* ---------- Hero ---------- */
    .hero {
      height: 100vh; /* slightly shorter so more of the image fits */
      min-height: 400px;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      text-align: center;
      background: url("assets/hero.png") center / cover no-repeat;
      position: relative;
      color: #fff;
    }

    /* On tall‑narrow screens show entire image without cropping */
    @media (max-aspect-ratio: 16/9) {
      .hero { background-size: contain; }
    }
    .hero::after {
      content: "";
      position: absolute;
      inset: 0;
      background: rgba(0, 0, 0, 0.4);
      /* backdrop-filter: blur(2px); */
    }
    .hero > * { position: relative; z-index: 1; }
    h1 {
      font-size: clamp(2.5rem, 5vw, 4rem);
      margin: 0 0 1rem;
      font-weight: 800;
    }

    /* ---------- Sections ---------- */
    h2 {
      font-size: 2rem;
      margin: 0 0 1.5rem;
      font-weight: 700;
      text-align: center;
      color: var(--accent);
    }
    section {
      padding: 4rem 1rem;
      max-width: 1200px;
      margin: auto;
    }
    h3 a {
      text-decoration: none;   /* 去下划线 */
      color: inherit;          /* 继承父元素颜色 */
    }
    h3 a:hover {
      text-decoration: underline;  /* 悬停时再显示下划线 */
    }

    /* ---------- Buttons ---------- */
    .btn {
      display: inline-block;
      padding: 0.75rem 1.5rem;
      border-radius: 9999px;
      background: var(--accent);
      color: #fff;
      text-decoration: none;
      font-weight: 600;
      transition: opacity 0.2s;
    }
    .btn:hover { opacity: 0.85; }

    /* ---------- Card Grid ---------- */
    .cardgrid {
      display: grid;
      grid-template-columns: repeat(5, 1fr);
      gap: 1.5rem;
      justify-items: center;
    }
    .card {
      background: #fff;
      border-radius: 1rem;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.05);
      padding: 1rem;
      text-align: center;
      width: 100%;
      max-width: 200px;
    }
    .card img {
      width: 96px;
      height: 96px;
      border-radius: 50%;
      object-fit: cover;
      margin-bottom: 1rem;
    }

    /* ---------- Timeline ---------- */
    .timeline {
      position: relative;
      border-left: 2px solid var(--accent);
      padding-left: 1.5rem;
      list-style: none;
      max-width: 500px;
      margin: auto;
    }
    .timeline li {
      margin: 0 0 1.5rem;
      position: relative;
    }
    .timeline li::before {
      content: "";
      position: absolute;
      left: -34px; /* center dot on the 2px border */
      top: 2px;
      width: 18px;
      height: 18px;
      border-radius: 50%;
      background: var(--accent);
    }

    /* ---------- Footer ---------- */
    footer {
      padding: 2rem 1rem;
      text-align: center;
      font-size: 0.875rem;
      color: #666;
    }
  </style>
</head>
<body>
  <!-- ================= Hero Banner ================= -->
  <header class="hero">
    <h1 class="morph" data-target="SEA: Sustainability with Earth Observation & AI">
      <!-- SEA: Sustainability with Earth Observation &amp; AI -->
    </h1>

    <p style="max-width: 700px; margin: 0 auto 2rem;">
      <span>ICCV 2025 - Honolulu Hawaii</span><br>
      <span>October 19, 2025</span><br>
      <span>Exploring innovative AI solutions for a sustainable future through Earth observation</span>
    </p>
    <a href="#call" class="btn">Submit Your Paper</a>
  </header>

  <!-- ================= Main Content ================= -->
  <main>
    <!-- Introduction -->
    <section id="intro">
      <h2>Introduction</h2>
      <p>
        The workshop brings together researchers, practitioners, and policy‑makers to advance the
        state‑of‑the‑art in applying artificial intelligence to Earth observation for sustainability
        challenges. Technically, this workshop explores how state-of-the-art EO data-tailored foundation models, efficient architectures, and novel learning paradigms can be leveraged or adapted to tackle pressing sustainability challenges.
        Topics include, but are not limited to, climate monitoring, disaster response, biodiversity, agriculture, urban development, clean energy, and social economics.
      </p>
    </section>

    <!-- Call for Papers -->
    <section id="call">
      <h2>Call for Papers</h2>
      The SEA Workshop offers two submission tracks: Proceedings and Non-Proceedings.
      All accepted papers will be showcased in the poster session, but only a selected subset from the Proceedings Track will be invited for oral presentations and considered for the Best Paper Award.
      <ul>
        <li><strong>Submission deadline:</strong> June 22, 2025 (AoE)</li>
        <li>Proceedings Track: 4-8 page papers (excluding references) using <a href="https://media.eventhosts.cc/Conferences/ICCV2025/ICCV2025-Author-Kit-Feb.zip">ICCV format</a>; Double-blind review.</li>
        <ul>
          <li>Accepted papers appear in ICCV Workshop Proceedings</li>
          <li>Best Paper Award will be chosen from submissions in the Proceedings Track.</li>
        </ul>
        <li>Non-Proceedings Track: allow extended abstracts or published work (up to 4-page papers) using any format; Double-blind review.</li>
      </ul>
      <p>
        <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/SEA" class="btn" target="_blank"
          >Submit via OpenReview</a
        >
      </p>
    </section>

    <!-- Invited Speakers -->
    <section id="speakers">
      <h2>Invited Speakers</h2>
      <div class="cardgrid">
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=DJ2KOn8AAAAJ&citpid=7" alt="naoto" />
          <h3><a href="https://naotoyokoya.com/">Naoto Yokoya</a></h3>
          <p>The University of Tokyo</p>
        </div>

        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=lJ_oh2EAAAAJ&citpid=5" alt="pio" />
          <h3><a href="https://scholar.google.com/citations?user=lJ_oh2EAAAAJ&hl=en">Piotr Bojanowski</a></h3>
          <p>Meta AI Research (DINOv3 team)</p>
        </div>

        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=1v9ooFUAAAAJ&citpid=1" alt="Wenzhe" />
          <h3><a href="https://eccb.tamu.edu/people/jiao-wenzhe/">Wenzhe Jiao</a></h3>
          <p>Texas A&M University</p>
        </div>
        
        <!-- <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=_L1i3RgAAAAJ&citpid=2" alt="Yelu Zeng" />
          <h3><a href="https://clst.cau.edu.cn/art/2021/11/12/art_31196_796403.html">Yelu Zeng</a></h3>
          <p>China Agricultural University</p>
        </div> -->
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=rBv-4qEAAAAJ&citpid=3" alt="Daniel" />
          <h3><a href="http://www.dancusworth.com/">Daniel Cusworth</a></h3>
          <p>Carbon Mapper/NASA JPL</p>
        </div>
        
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=Umm6tPQAAAAJ&citpid=3" alt="chr" />
          <h3><a href="https://scholar.google.com/citations?user=Umm6tPQAAAAJ&hl=en">Christopher F. Brown</a></h3>
          <p>Google DeepMind (AlphaEarth team)</p>
        </div>
      </div>
    </section>

    <!-- Accepted Papers -->
    <section id="accepted">
      <h2>Accepted Papers</h2>
      <ul>
        <li>Adhemar de Senneville, Xavier Bou, Jean-Louis Bonne, Nicolas Dumelie, Rafael Grompone von Gioi, Thibaud Ehret, Gabriele Facciolo. <strong>Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection</strong> (oral)</li>
        <li>Toqi Tahamid Sarker, Mohamed Embaby, Taminul Islam, AMER ABUGHAZALEH, Khaled Ahmed. <strong>GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging</strong> (oral)</li>
        <li>Angela Tsao, David B. Lobell. <strong>PlantationBench: a multiscale, multimodal remote sensing benchmark for tree plantation mapping under distribution shift</strong> (oral)</li>
        <li>Takayuki Shinohara, Hidetaka Saomoto. <strong>ViT-Koop: Vision-Transformer–Koopman Operators for Efficient Time-Series Forecasting of Earth-Observation Data</strong></li>
        <li>Yue Zhou, Mengcheng Lan, Xiang Li, Litong Feng, Yiping Ke, Xue Jiang, Qingyun Li, Xue Yang, Wayne Zhang. <strong>GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding</strong></li>
        <li>Aditya Chakravarty. <strong>Out-of-Distribution Generalization in Climate-Aware Crop Yield Prediction with Earth Observation Data</strong></li>
        <li>Nicolas Drapier, Aladine Chetouani, Aurélien Chateigner. <strong>Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery</strong></li>
        <li>Wei Lu, Si-Bao Chen, Huidong Li, Qingling Shu, Chris Ding, Jin Tang, Bin Luo. <strong>LEGNet: A Lightweight Edge-Gaussian Network for Low-Quality Remote Sensing Image Object Detection</strong></li>
        <li>Yahya Ibrahim, Márta Belényesi, Chang Liu, Mátyás Richter-Cserey, Máté Simon, Tamas Sziranyi, Csaba Benedek. <strong>Inland Excess Water (IEW) Monitoring Using Sentinel-1/2: A SplitClass Segmentation and Temporal Gap-Filling Approach</strong></li>
        <li>Zhenghui Zhao, Chen Wu, Di Wang, Hongruixuan Chen, Zhuo Zheng. <strong>ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing</strong></li>
        <li>Philip Wootaek Shin, Vishal Gaur, Rahul Ramachandran, Manil Maskey, Jack Sampson, Vijaykrishnan Narayanan, Sujit Roy. <strong>Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery</strong></li>
        <li>Yuchi Ma, Yuval Sadeh, Sheila Baber, Oleksandra Oliinyk, Inbal Becker-Reshef, David B. Lobell. <strong>Transfer Learning-based winter wheat yield mapping in Ukraine</strong></li>
        <li>Mark Moussa, Andre Williams, Seth Roffe, Douglas C Morton. <strong>PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery</strong></li>
        <li>Jess Tam, William K Cornwell. <strong>Simple edge-guided wildlife classification with classical detectors</strong></li>
        <li>Shengjie Liu, Lu Zhang, Siqin Wang. <strong>Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction</strong></li>
        <li>Subin Varghese, Joshua Gao, Vedhus Hoskere. <strong>ViewDelta: Scaling Scene Change Detection through Text-Conditioning</strong></li>
        <li>Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, WEI YiMin, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya. <strong>BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response</strong></li>
        <li>Phuc Nguyen. <strong>HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection</strong></li>
        <li>Xiaoyan Lu, Qihao Weng. <strong>Tree Mapping with Limited Data: Fine-Tuning Foundation Models for Multimodal Fusion</strong></li>
        <li>Vishal Nedungadi, Xingguo Xiong, Aike Potze, Ron van Bree, Tao Lin, Marc Rußwurm, Ioannis N. Athanasiadis. <strong>From General to Specialized: The Need for Foundational Models in Agriculture</strong></li>
        <li>Abhiroop Chatterjee, Susmita Ghosh, Ashish Ghosh. <strong>Context-Aware Masking and Learnable Diffusion-Guided Patch Refinement in Transformers via Sparse Supervision for Hyperspectral Image Classification</strong></li>
        <li>Om A. Desai, Siddhant Desai, Darshan A. Desai. <strong>MetaChange: A Risk-Adjusted Foundation Model for Global Afforestation & Beyond</strong></li>
        <li>Songkun Yan, Zhi Li, Siyu Zhu, Yixin Wen, Mofan Zhang, Mengye Chen, Jie Cao, Yang Hong. <strong>AQUAH: Automatic Quantification and Unified Agent in Hydrology</strong></li>
        <li>Zhuoning Gu, Xiao-Peng Song. <strong>A Transformer-based deep learning network for barley and wheat mapping using time-series Sentinel-2 imagery</strong></li>
        <li>Jason Blake Cohen. <strong>Improving global black carbon radiative forcing using satellites, physical models, and machine learning in tandem</strong></li>
        <li>Prasanth. <strong>BiodiverseNet: Multitask Learning on Fused Multispectral and Radar Data for Scalable Ecosystem Monitoring</strong></li>
        <li>Jospeh Chai, Hoang D. Nguyen, Barry O' Sulllivan. <strong>IRLTrees3D: A 3D Reconstruction Dataset of Trees</strong></li>
        <li>Siddharth Sachdeva, Sidharth Tadeparti, David B. Lobell. <strong>Cross-view Object Geolocalization for Tree Species Mapping</strong></li>
        <li>Judah Goldfeder, Gabriel Guerra Trigo, Philippe Martin Wyder, Neil Kachappilly, Hod Lipson. <strong>Evaluating Performance of Reinforcement Learning Agents to Control Buildings Efficiently</strong></li>
      </ul>
    </section>

    <!-- Organizers -->
    <section id="organizers">
      <h2>Organizers</h2>
      <div class="cardgrid">
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=CREpn_AAAAAJ&citpid=7" alt="Zhuo Zheng" />
          <h3><a href="https://zhuozheng.top">Zhuo Zheng</a></h3>
          <p>Stanford University</p>
        </div>
        <div class="card">
          <img src="https://junjuewang.top/img/myphoto24.jpg" alt="Junjue Wang" />
          <h3><a href="https://junjuewang.top/">Junjue Wang</a></h3>
          <p>The University of Tokyo</p>
        </div>
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=MDA37NMAAAAJ&citpid=18" alt="Xiaoyan Lu" />
          <h3><a href="https://xiaoyan07.github.io/">Xiaoyan Lu</a></h3>
          <p>The Hong Kong Polytechnic University</p>
        </div>
        <div class="card">
          <img src="https://energypostdoc.stanford.edu/sites/sepf/files/styles/medium_square/public/media/image/17211705667282_.pic_hd_1_0.jpg?h=c761230c&itok=V5WrvcDt" alt="Xinyu Dou" />
          <h3><a href="https://scholar.google.com/citations?hl=en&user=Gv3iHJUAAAAJ&view_op=list_works">Xinyu Dou</a></h3>
          <p>Stanford University</p>
        </div>
        <div class="card">
          <img src="https://gengchenmai.github.io/img/selfie/Gengchen_Mai-2018-12.JPG" alt="Gengchen Mai" />
          <h3><a href="https://gengchenmai.github.io/">Gengchen Mai</a></h3>
          <p>University of Texas at Austin</p>
        </div>
        <div class="card">
          <img src="assets/yanfei.jpg" alt="Yanfei Zhong" />
          <h3><a href="https://rsidea.whu.edu.cn/index.html">Yanfei Zhong</a></h3>
          <p>Wuhan University</p>
        </div>
        <div class="card">
          <img src="assets/liangpei.jpg" alt="Liangpei Zhang" />
          <h3><a href="http://www.lmars.whu.edu.cn/prof_web/zhangliangpei/rs/index.html">Liangpei Zhang</a></h3>
          <p>Wuhan University</p>
        </div>
        <div class="card">
          <img src="https://web.stanford.edu/~mburke/Marshall_Burke.jpg" alt="Marshall Burke" />
          <h3><a href="https://web.stanford.edu/~mburke">Marshall Burke</a></h3>
          <p>Stanford University</p>
        </div>
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=biuzU-AAAAAJ&citpid=2" alt="David Lobell" />
          <h3><a href="https://scholar.google.com/citations?user=biuzU-AAAAAJ">David Lobell</a></h3>
          <p>Stanford University</p>
        </div>
        <div class="card">
          <img src="https://cs.stanford.edu/~ermon/IMG_8217_s_1.jpg" alt="Stefano Ermon" />
          <h3><a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a></h3>
          <p>Stanford University</p>
        </div>
      </div>
    </section>

    <!-- Timeline -->
    <section id="timeline">
      <h2>Timeline</h2>
      <ul class="timeline">
        <li>
          <strong>Paper Submission Deadline</strong><br />
          <del>June 4, 2025</del>
          June 22, 2025
        </li>
        <li>
          <strong>Notification to Authors</strong><br />
          <del>June 25, 2025</del>
          July 9, 2025
        </li>
        <li>
          <strong>Camera‑Ready Deadline</strong><br />
          <del>July 11, 2025</del>
          August 14, 2025
        </li>
        <li>
          <strong>Workshop at ICCV</strong><br />
          October 19, 2025
        </li>
      </ul>
    </section>
    <section id="sponsor">
      <h2>Sponsorship</h2>
      We are seeking sponsors to help fund travel grants, the best paper award, and other workshop initiatives. If you or your organization is interested in supporting SEA, please reach out to the organizing team via email (sea_iccv2025@googlegroups.com).
    </section>
  </main>

  <!-- ================= Footer ================= -->
  <footer>
    © ICCV 2025 Sustainability with Earth Observation &amp; AI Workshop. All rights reserved.
  </footer>
  <script>
    /* random card diffusion positions */
    document.addEventListener('DOMContentLoaded', () => {
      /* Discrete‑to‑target morphing */
      const alphabet='A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z'.split('');
      function scramble(len){let s='';for(let i=0;i<len;i++){s+=alphabet[Math.floor(Math.random()*alphabet.length)];}return s;}
      document.querySelectorAll('.morph').forEach((el)=>{
        const target=el.getAttribute('data-target');
        let frame=0,maxFrame=15; // 
        const interval=setInterval(()=>{
          if(frame>=maxFrame){el.textContent=target;clearInterval(interval);return;}
          let revealCount=Math.floor(target.length*frame/maxFrame);
          let shown=target.slice(0,revealCount)+scramble(target.length-revealCount);
          el.textContent=shown;
          el.style.fontFamily='var(--font-mono)';
          frame++;
        },60);
      });
    });
  </script>
</body>
</html>
