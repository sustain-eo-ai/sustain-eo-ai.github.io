<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SEA – ICCV 2025 Workshop</title>
  <style>
    /* :root { --accent: #10b981; } */
    :root { --accent: #0ABAB5; }
    * { box-sizing: border-box; scroll-behavior: smooth; }
    body {
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
      line-height: 1.6;
      color: #111;
      background: #f8fafc;
    }

    /* ---------- Hero ---------- */
    .hero {
      height: 100vh; /* slightly shorter so more of the image fits */
      min-height: 400px;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      text-align: center;
      background: url("assets/hero.png") center / cover no-repeat;
      position: relative;
      color: #fff;
    }

    /* On tall‑narrow screens show entire image without cropping */
    @media (max-aspect-ratio: 16/9) {
      .hero { background-size: contain; }
    }
    .hero::after {
      content: "";
      position: absolute;
      inset: 0;
      background: rgba(0, 0, 0, 0.4);
      /* backdrop-filter: blur(2px); */
    }
    .hero > * { position: relative; z-index: 1; }
    h1 {
      font-size: clamp(2.5rem, 5vw, 4rem);
      margin: 0 0 1rem;
      font-weight: 800;
    }

    /* ---------- Sections ---------- */
    h2 {
      font-size: 2rem;
      margin: 0 0 1.5rem;
      font-weight: 700;
      text-align: center;
      color: var(--accent);
    }
    section {
      padding: 4rem 1rem;
      max-width: 1200px;
      margin: auto;
    }
    h3 a {
      text-decoration: none;   /* 去下划线 */
      color: inherit;          /* 继承父元素颜色 */
    }
    h3 a:hover {
      text-decoration: underline;  /* 悬停时再显示下划线 */
    }

    /* ---------- Buttons ---------- */
    .btn {
      display: inline-block;
      padding: 0.75rem 1.5rem;
      border-radius: 9999px;
      background: var(--accent);
      color: #fff;
      text-decoration: none;
      font-weight: 600;
      transition: opacity 0.2s;
    }
    .btn:hover { opacity: 0.85; }

    /* ---------- Card Grid ---------- */
    .cardgrid {
      display: grid;
      grid-template-columns: repeat(5, 1fr);
      gap: 1.5rem;
      justify-items: center;
    }
    .card {
      background: #fff;
      border-radius: 1rem;
      box-shadow: 0 4px 10px rgba(0, 0, 0, 0.05);
      padding: 1rem;
      text-align: center;
      width: 100%;
      max-width: 200px;
    }
    .card img {
      width: 96px;
      height: 96px;
      border-radius: 50%;
      object-fit: cover;
      margin-bottom: 1rem;
    }

    /* ---------- Timeline ---------- */
    .timeline {
      position: relative;
      border-left: 2px solid var(--accent);
      padding-left: 1.5rem;
      list-style: none;
      max-width: 500px;
      margin: auto;
    }
    .timeline li {
      margin: 0 0 1.5rem;
      position: relative;
    }
    .timeline li::before {
      content: "";
      position: absolute;
      left: -34px; /* center dot on the 2px border */
      top: 2px;
      width: 18px;
      height: 18px;
      border-radius: 50%;
      background: var(--accent);
    }

    /* ---------- Footer ---------- */
    footer {
      padding: 2rem 1rem;
      text-align: center;
      font-size: 0.875rem;
      color: #666;
    }
  
/* ---- Speaker Cards Adjustments ---- */
#speakers .cardgrid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
  gap: 1.5rem;
  justify-items: stretch;
  max-width: 1200px;
  margin: auto;
}
#speakers .card {
  display: flex;
  flex-direction: column;
  justify-content: flex-start;
  background: #fff;
  border-radius: 1rem;
  box-shadow: 0 4px 10px rgba(0,0,0,0.05);
  padding: 1rem;
  text-align: justify;
  min-height: 450px;
}
#speakers .card h3 {
  text-align: center;
  margin-top: 0.5rem;
  margin-bottom: 0.5rem;
}
#speakers .card img {
  width: 96px;
  height: 96px;
  border-radius: 50%;
  object-fit: cover;
  margin: 0 auto 1rem;
}


/* ---- Speaker Cards Vertical Layout ---- */
#speakers .cardgrid {
  display: flex;
  flex-direction: column;
  gap: 2rem;
  max-width: 800px;
  margin: auto;
}
#speakers .card {
  display: flex;
  flex-direction: row;
  align-items: flex-start;
  background: #fff;
  border-radius: 1rem;
  box-shadow: 0 4px 10px rgba(0,0,0,0.05);
  padding: 1.5rem;
  min-height: auto;
  text-align: justify;
}
#speakers .card img {
  width: 120px;
  height: 120px;
  border-radius: 50%;
  object-fit: cover;
  margin-right: 1.5rem;
}
#speakers .card h3 {
  margin-top: 0;
  margin-bottom: 0.5rem;
  text-align: left;
}
#speakers .card p {
  margin: 0;
}


/* ---- Speaker Cards Fixed Avatar Column Layout ---- */
#speakers .card {
  display: grid;
  grid-template-columns: 140px 1fr;
  gap: 1.5rem;
  align-items: start;
  background: #fff;
  border-radius: 1rem;
  box-shadow: 0 4px 10px rgba(0,0,0,0.05);
  padding: 1.5rem;
  text-align: justify;
}
#speakers .card img {
  width: 120px;
  height: 120px;
  border-radius: 50%;
  object-fit: cover;
  margin: 0 auto;
}
#speakers .card h3 {
  margin-top: 0;
  margin-bottom: 0.5rem;
  text-align: left;
}
#speakers .card p {
  margin: 0;
}


/* ---- Speaker Section Width Adjustment ---- */
#speakers {
  max-width: 900px;
  margin: auto;
  padding: 2rem 1rem;
}
#speakers .cardgrid {
  display: flex;
  flex-direction: column;
  gap: 2rem;
}
#speakers .card {
  display: grid;
  grid-template-columns: 140px 1fr;
  gap: 1.5rem;
  align-items: start;
  background: #fff;
  border-radius: 1rem;
  box-shadow: 0 4px 10px rgba(0,0,0,0.05);
  padding: 1.5rem;
  text-align: justify;
  width: 100%;
}


/* ---- Final Speaker Width Fix ---- */
#speakers {
  max-width: 1200px !important;
  margin: auto !important;
  padding: 4rem 1rem !important;
}
#speakers .cardgrid {
  max-width: 100% !important;
  width: 100% !important;
  margin: 0 auto !important;
}


/* ---- Invited Speakers: One-per-row layout ---- */
#speakers .speaker-list {
  display: flex;
  flex-direction: column;
  gap: 2rem;
}
#speakers .speaker-row {
  display: grid;
  grid-template-columns: 140px 1fr;
  gap: 1.25rem;
  align-items: start;
  background: #fff;
  border-radius: 1rem;
  box-shadow: 0 4px 10px rgba(0,0,0,0.05);
  padding: 1.25rem;
}
#speakers .speaker-row img {
  width: 120px;
  height: 120px;
  border-radius: 50%;
  object-fit: cover;
  margin: 0 auto;
}
#speakers .speaker-content h3 {
  margin: 0 0 0.35rem;
  font-size: 1.1rem;
  text-align: left;
}
#speakers .speaker-content p {
  margin: 0.25rem 0 0;
  text-align: justify;
}
@media (max-width: 640px) {
  #speakers .speaker-row { grid-template-columns: 100px 1fr; }
  #speakers .speaker-row img { width: 90px; height: 90px; }
}

</style>

<style>

/* ---- Invited Speakers Schedule Table ---- */
.schedule-wrap {
  margin-top: 2rem;
  max-width: 1200px;
  margin-left: auto;
  margin-right: auto;
  padding: 0 1rem;
}
.schedule-wrap h3 {
  margin: 0 0 1rem;
  text-align: center;
  color: var(--accent, #0a6);
  font-size: 1.25rem;
}
.schedule-table {
  width: 100%;
  border-collapse: collapse;
  background: #fff;
  border-radius: 12px;
  overflow: hidden;
  box-shadow: 0 4px 10px rgba(0,0,0,.05);
  font-size: 0.95rem;
}
.schedule-table th, .schedule-table td {
  padding: 0.9rem 1rem;
  border-bottom: 1px solid #eef2f7;
  text-align: left;
  vertical-align: top;
}
.schedule-table thead th {
  background: #f1f5f9;
  font-weight: 700;
}
.schedule-table tr:last-child td { border-bottom: none; }
.schedule-table .time { white-space: nowrap; font-weight: 600; }
.schedule-table .talk-title { font-weight: 600; }
@media (max-width: 720px) {
  .schedule-table { display: block; overflow-x: auto; }
}

</style>

</head>
<body>
  <!-- ================= Hero Banner ================= -->
  <header class="hero">
    <h1 class="morph" data-target="SEA: Sustainability with Earth Observation & AI">
      <!-- SEA: Sustainability with Earth Observation &amp; AI -->
    </h1>

    <p style="max-width: 700px; margin: 0 auto 2rem;">
      <span>ICCV 2025 - Honolulu Hawaii</span><br>
      <span>October 19, 2025</span><br>
      <span>Exploring innovative AI solutions for a sustainable future through Earth observation</span>
    </p>
    <a href="#call" class="btn">Submit Your Paper</a>
  </header>

  <!-- ================= Main Content ================= -->
  <main>
    <!-- Introduction -->
    <section id="intro">
      <h2>Introduction</h2>
      <p>
        The workshop brings together researchers, practitioners, and policy‑makers to advance the
        state‑of‑the‑art in applying artificial intelligence to Earth observation for sustainability
        challenges. Technically, this workshop explores how state-of-the-art EO data-tailored foundation models, efficient architectures, and novel learning paradigms can be leveraged or adapted to tackle pressing sustainability challenges.
        Topics include, but are not limited to, climate monitoring, disaster response, biodiversity, agriculture, urban development, clean energy, and social economics.
      </p>
    </section>

    <!-- Call for Papers -->
    <section id="call">
      <h2>Call for Papers</h2>
      The SEA Workshop offers two submission tracks: Proceedings and Non-Proceedings.
      All accepted papers will be showcased in the poster session, but only a selected subset from the Proceedings Track will be invited for oral presentations and considered for the Best Paper Award.
      <ul>
        <li><strong>Submission deadline:</strong> June 22, 2025 (AoE)</li>
        <li>Proceedings Track: 4-8 page papers (excluding references) using <a href="https://media.eventhosts.cc/Conferences/ICCV2025/ICCV2025-Author-Kit-Feb.zip">ICCV format</a>; Double-blind review.</li>
        <ul>
          <li>Accepted papers appear in ICCV Workshop Proceedings</li>
          <li>Best Paper Award will be chosen from submissions in the Proceedings Track.</li>
        </ul>
        <li>Non-Proceedings Track: allow extended abstracts or published work (up to 4-page papers) using any format; Double-blind review.</li>
      </ul>
      <p>
        <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/SEA" class="btn" target="_blank"
          >Submit via OpenReview</a>
      </p>
    </section>

    <div class="schedule-wrap" id="speaker-schedule">
  <h3>Agenda (Oct 19, 2025 · 307 A · 1–5 pm, Hawaii Time UTC−10)</h3>
  <table class="schedule-table">
    <thead>
      <tr>
        <th style="width:12ch;">Time</th>
        <th>Session</th>
        <th>Speaker</th>
        <th>Title</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="time">13:00-13:10</td>
        <td class="talk-title">Opening &amp; Welcome</td>
        <td>SEA Organizers</td>
        <td> - </td>
      </tr>
      <tr>
        <td class="time">13:10-13:50</td>
        <td class="talk-title">Invited Talk</td>
        <td>Naoto Yokoya</td>
        <td>Open and Equitable AI for Earth Observation</td>
      </tr>
      <tr>
        <td class="time">13:50-14:30</td>
        <td class="talk-title">Invited Talk</td>
        <td>Christopher F. Brown</td>
        <td>Efficiently Exploiting Full EO Archives Through Embedding Methods</td>
      </tr>
      <tr>
        <td class="time">14:30-14:40</td>
        <td class="talk-title">Oral presentation</td>
        <td>Angela Tsao</td>
        <td>PlantationBench: a multiscale, multimodal remote sensing benchmark for tree plantation mapping under distribution shift</td>
      </tr>
      <tr>
        <td class="time">14:40-14:50</td>
        <td class="talk-title">Oral presentation</td>
        <td>Toqi Tahamid Sarker</td>
        <td>GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging</td>
      </tr>
      <tr>
        <td class="time">14:50-15:00</td>
        <td class="talk-title">Oral presentation</td>
        <td>Adhemar de Senneville</td>
        <td>Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection</td>
      </tr>
      <tr>
        <td class="time">15:00-15:30</td>
        <td class="talk-title">Coffee Break</td>
        <td>—</td>
        <td>Coffee &amp; posters</td>
      </tr>
      <tr>
        <td class="time">15:30-16:10</td>
        <td class="talk-title">Invited Talk</td>
        <td>Daniel Cusworth</td>
        <td>The Role of Remote Sensing and AI to Reduce Methane Emissions</td>
      </tr>
      <tr>
        <td class="time">16:10-16:50</td>
        <td class="talk-title">Invited Talk</td>
        <td>Piotr Bojanowski</td>
        <td>Self-supervised representation learning and remote sensing</td>
      </tr>
      
      <tr>
        <td class="time">16:50-17:00</td>
        <td class="talk-title">Closing</td>
        <td>SEA Organizers</td>
        <td>Awards &amp; wrap-up</td>
      </tr>
    </tbody>
  </table>
</div>

    <!-- Invited Speakers -->
    
<section id="speakers">
  <h2>Invited Speakers</h2>
  <div class="speaker-list">
    <div class="speaker-row">
      <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=DJ2KOn8AAAAJ&citpid=7" alt="Naoto Yokoya">
      <div class="speaker-content">
        <h3><a href="https://naotoyokoya.com/">Naoto Yokoya</a></h3>
        <p><strong>Professor, University of Tokyo (Graduate School of Frontier Sciences)</strong>; leads the Geoinformatics Team at RIKEN AIP. He received his Ph.D. in aerospace engineering from the University of Tokyo in 2013. His research lies at the intersection of remote sensing and computer vision, with applications to disaster management and environmental assessment. He previously held an Alexander von Humboldt Fellowship at DLR/TUM and currently serves as Associate Editor for IEEE TPAMI, IEEE TGRS, and ISPRS JPRS; he is a Clarivate Highly Cited Researcher (2022–).</p>
      </div>
    </div>

    <div class="speaker-row">
      <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=Umm6tPQAAAAJ&citpid=3" alt="Christopher F. Brown">
      <div class="speaker-content">
        <h3><a href="https://scholar.google.com/citations?user=Umm6tPQAAAAJ&hl=en">Christopher F. Brown</a></h3>
        <p><strong>Senior Research Engineer, DeepMind</strong>, leading research at the intersection of AI and Earth observation. His work focuses on developing and applying large‑scale AI models to address global environmental challenges. Christopher has led several projects, including AlphaEarth, Cloud Score+, and Dynamic World. His research aims to provide the scientific community with new tools to better monitor, model, and protect Earth's systems.</p>
      </div>
    </div>

    <div class="speaker-row">
      <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=rBv-4qEAAAAJ&citpid=3" alt="Daniel Cusworth">
      <div class="speaker-content">
        <h3><a href="http://www.dancusworth.com/">Daniel Cusworth</a></h3>
        <p><strong>Science Director, Carbon Mapper</strong>. Carbon Mapper’s mission is to drive greenhouse gas emission reductions by making methane and carbon dioxide data accessible and actionable. He was formerly a Data Scientist at the NASA Jet Propulsion Laboratory and a Research Scientist at the University of Arizona and worked on quantification of anthropogenic carbon dioxide and methane emissions from regional to facility scales. He received his B.S. in Applied Math/Atmospheric Sciences at UCLA and Ph.D. in Atmospheric Chemistry at Harvard University.</p>
      </div>
    </div>

    <div class="speaker-row">
      <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=lJ_oh2EAAAAJ&citpid=5" alt="Piotr Bojanowski">
      <div class="speaker-content">
        <h3><a href="https://scholar.google.com/citations?user=lJ_oh2EAAAAJ&hl=en">Piotr Bojanowski</a></h3>
        <p><strong>Research Scientist Director, Meta FAIR</strong>. Before joining Meta FAIR in 2016, he completed a PhD in computer vision and machine learning at École Normale Supérieure, under the supervision of Jean Ponce, Cordelia Schmidt, Ivan Laptev, and Josef Sivic. He was a key contributor to FastText, a library for efficient learning of word representations and text classification. His work focuses on large‑scale unsupervised and self‑supervised learning in computer vision, and he led the development of DINOv2 and DINOv3—state‑of‑the‑art large vision models trained without human supervision.</p>
      </div>
    </div>
  </div>
</section>


    <!-- Accepted Papers -->
    <section id="accepted">
      <h2>Accepted Papers</h2>
      <ul>
        <li>Adhemar de Senneville, Xavier Bou, Jean-Louis Bonne, Nicolas Dumelie, Rafael Grompone von Gioi, Thibaud Ehret, Gabriele Facciolo. <strong>Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection</strong> (oral)</li>
        <li>Toqi Tahamid Sarker, Mohamed Embaby, Taminul Islam, AMER ABUGHAZALEH, Khaled Ahmed. <strong>GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging</strong> (oral)</li>
        <li>Angela Tsao, David B. Lobell. <strong>PlantationBench: a multiscale, multimodal remote sensing benchmark for tree plantation mapping under distribution shift</strong> (oral)</li>
        <li>Takayuki Shinohara, Hidetaka Saomoto. <strong>ViT-Koop: Vision-Transformer–Koopman Operators for Efficient Time-Series Forecasting of Earth-Observation Data</strong></li>
        <li>Yue Zhou, Mengcheng Lan, Xiang Li, Litong Feng, Yiping Ke, Xue Jiang, Qingyun Li, Xue Yang, Wayne Zhang. <strong>GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding</strong></li>
        <li>Aditya Chakravarty. <strong>Out-of-Distribution Generalization in Climate-Aware Crop Yield Prediction with Earth Observation Data</strong></li>
        <li>Nicolas Drapier, Aladine Chetouani, Aurélien Chateigner. <strong>Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery</strong></li>
        <li>Wei Lu, Si-Bao Chen, Huidong Li, Qingling Shu, Chris Ding, Jin Tang, Bin Luo. <strong>LEGNet: A Lightweight Edge-Gaussian Network for Low-Quality Remote Sensing Image Object Detection</strong></li>
        <li>Yahya Ibrahim, Márta Belényesi, Chang Liu, Mátyás Richter-Cserey, Máté Simon, Tamas Sziranyi, Csaba Benedek. <strong>Inland Excess Water (IEW) Monitoring Using Sentinel-1/2: A SplitClass Segmentation and Temporal Gap-Filling Approach</strong></li>
        <li>Zhenghui Zhao, Chen Wu, Di Wang, Hongruixuan Chen, Zhuo Zheng. <strong>ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing</strong></li>
        <li>Philip Wootaek Shin, Vishal Gaur, Rahul Ramachandran, Manil Maskey, Jack Sampson, Vijaykrishnan Narayanan, Sujit Roy. <strong>Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery</strong></li>
        <li>Yuchi Ma, Yuval Sadeh, Sheila Baber, Oleksandra Oliinyk, Inbal Becker-Reshef, David B. Lobell. <strong>Transfer Learning-based winter wheat yield mapping in Ukraine</strong></li>
        <li>Mark Moussa, Andre Williams, Seth Roffe, Douglas C Morton. <strong>PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery</strong></li>
        <li>Jess Tam, William K Cornwell. <strong>Simple edge-guided wildlife classification with classical detectors</strong></li>
        <li>Shengjie Liu, Lu Zhang, Siqin Wang. <strong>Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction</strong></li>
        <li>Subin Varghese, Joshua Gao, Vedhus Hoskere. <strong>ViewDelta: Scaling Scene Change Detection through Text-Conditioning</strong></li>
        <li>Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, Weihao Xuan, Junjue Wang, Xinlei Shao, WEI YiMin, Junshi Xia, Cuiling Lan, Konrad Schindler, Naoto Yokoya. <strong>BRIGHT: A globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response</strong></li>
        <li>Phuc Nguyen. <strong>HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection</strong></li>
        <li>Xiaoyan Lu, Qihao Weng. <strong>Tree Mapping with Limited Data: Fine-Tuning Foundation Models for Multimodal Fusion</strong></li>
        <li>Vishal Nedungadi, Xingguo Xiong, Aike Potze, Ron van Bree, Tao Lin, Marc Rußwurm, Ioannis N. Athanasiadis. <strong>From General to Specialized: The Need for Foundational Models in Agriculture</strong></li>
        <li>Abhiroop Chatterjee, Susmita Ghosh, Ashish Ghosh. <strong>Context-Aware Masking and Learnable Diffusion-Guided Patch Refinement in Transformers via Sparse Supervision for Hyperspectral Image Classification</strong></li>
        <li>Om A. Desai, Siddhant Desai, Darshan A. Desai. <strong>MetaChange: A Risk-Adjusted Foundation Model for Global Afforestation & Beyond</strong></li>
        <li>Songkun Yan, Zhi Li, Siyu Zhu, Yixin Wen, Mofan Zhang, Mengye Chen, Jie Cao, Yang Hong. <strong>AQUAH: Automatic Quantification and Unified Agent in Hydrology</strong></li>
        <li>Zhuoning Gu, Xiao-Peng Song. <strong>A Transformer-based deep learning network for barley and wheat mapping using time-series Sentinel-2 imagery</strong></li>
        <li>Jason Blake Cohen. <strong>Improving global black carbon radiative forcing using satellites, physical models, and machine learning in tandem</strong></li>
        <li>Prasanth. <strong>BiodiverseNet: Multitask Learning on Fused Multispectral and Radar Data for Scalable Ecosystem Monitoring</strong></li>
        <li>Jospeh Chai, Hoang D. Nguyen, Barry O' Sulllivan. <strong>IRLTrees3D: A 3D Reconstruction Dataset of Trees</strong></li>
        <li>Siddharth Sachdeva, Sidharth Tadeparti, David B. Lobell. <strong>Cross-view Object Geolocalization for Tree Species Mapping</strong></li>
        <li>Judah Goldfeder, Gabriel Guerra Trigo, Philippe Martin Wyder, Neil Kachappilly, Hod Lipson. <strong>Evaluating Performance of Reinforcement Learning Agents to Control Buildings Efficiently</strong></li>
      </ul>
    </section>

    <!-- Organizers -->
    <section id="organizers">
      <h2>Organizers</h2>
      <div class="cardgrid">
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=CREpn_AAAAAJ&citpid=7" alt="Zhuo Zheng" />
          <h3><a href="https://zhuozheng.top">Zhuo Zheng</a></h3>
          <p>Stanford University</p>
        </div>
        <div class="card">
          <img src="https://junjuewang.top/img/myphoto24.jpg" alt="Junjue Wang" />
          <h3><a href="https://junjuewang.top/">Junjue Wang</a></h3>
          <p>The University of Tokyo</p>
        </div>
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=MDA37NMAAAAJ&citpid=18" alt="Xiaoyan Lu" />
          <h3><a href="https://xiaoyan07.github.io/">Xiaoyan Lu</a></h3>
          <p>The Hong Kong Polytechnic University</p>
        </div>
        <div class="card">
          <img src="https://energypostdoc.stanford.edu/sites/sepf/files/styles/medium_square/public/media/image/17211705667282_.pic_hd_1_0.jpg?h=c761230c&itok=V5WrvcDt" alt="Xinyu Dou" />
          <h3><a href="https://scholar.google.com/citations?hl=en&user=Gv3iHJUAAAAJ&view_op=list_works">Xinyu Dou</a></h3>
          <p>Stanford University</p>
        </div>
        <div class="card">
          <img src="https://gengchenmai.github.io/img/selfie/Gengchen_Mai-2018-12.JPG" alt="Gengchen Mai" />
          <h3><a href="https://gengchenmai.github.io/">Gengchen Mai</a></h3>
          <p>University of Texas at Austin</p>
        </div>
        <div class="card">
          <img src="assets/yanfei.jpg" alt="Yanfei Zhong" />
          <h3><a href="https://rsidea.whu.edu.cn/index.html">Yanfei Zhong</a></h3>
          <p>Wuhan University</p>
        </div>
        <div class="card">
          <img src="assets/liangpei.jpg" alt="Liangpei Zhang" />
          <h3><a href="http://www.lmars.whu.edu.cn/prof_web/zhangliangpei/rs/index.html">Liangpei Zhang</a></h3>
          <p>Wuhan University</p>
        </div>
        <div class="card">
          <img src="https://web.stanford.edu/~mburke/Marshall_Burke.jpg" alt="Marshall Burke" />
          <h3><a href="https://web.stanford.edu/~mburke">Marshall Burke</a></h3>
          <p>Stanford University</p>
        </div>
        <div class="card">
          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=biuzU-AAAAAJ&citpid=2" alt="David Lobell" />
          <h3><a href="https://scholar.google.com/citations?user=biuzU-AAAAAJ">David Lobell</a></h3>
          <p>Stanford University</p>
        </div>
        <div class="card">
          <img src="https://cs.stanford.edu/~ermon/IMG_8217_s_1.jpg" alt="Stefano Ermon" />
          <h3><a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a></h3>
          <p>Stanford University</p>
        </div>
      </div>
    </section>

    <!-- Program Committee -->
    <section id="pc">
      <h2>Program Committee</h2>
      <ul class="pc-list">
        <li><span class="name">Ji Zhao</span><span class="affil">China University of Geoscience</span></li>
        <li><span class="name">Xiaofan Gui</span><span class="affil">Microsoft</span></li>
        <li><span class="name">Dino Ienco</span><span class="affil">National Institute for Agriculture, Environment and Food</span></li>
        <li><span class="name">Piyu Ke</span><span class="affil">Tsinghua University</span></li>
        <li><span class="name">Dingyuan Chen</span><span class="affil">Hangzhou Dianzi University</span></li>
        <li><span class="name">Jingtao Li</span><span class="affil">Wuhan University</span></li>
        <li><span class="name">Adam J Stewart</span><span class="affil">Technische Universität München</span></li>
        <li><span class="name">Hongruixuan Chen</span><span class="affil">The University of Tokyo</span></li>
        <li><span class="name">Xiaoyan Lu</span><span class="affil">The Hong Kong Polytechnic University</span></li>
        <li><span class="name">Zihang Chen</span><span class="affil">Wuhan University</span></li>
        <li><span class="name">Yiheng Zhou</span><span class="affil">Wuhan University</span></li>
        <li><span class="name">Jingjun Yi</span><span class="affil">Wuhan University</span></li>
        <li><span class="name">Meiqi Hu</span><span class="affil">Sun Yat-sen University</span></li>
        <li><span class="name">Yinhe Liu</span><span class="affil">Wuhan University</span></li>
        <li><span class="name">Yonghao Xu</span><span class="affil">Linköping University</span></li>
        <li><span class="name">Hengqi Wang</span><span class="affil">Chinese Academy of Sciences</span></li>
        <li><span class="name">Sebastian Hafner</span><span class="affil">University of Glasgow</span></li>
        <li><span class="name">Marc Rußwurm</span><span class="affil">Wageningen University</span></li>
      </ul>
    </section>

    <!-- Timeline -->
    <section id="timeline">
      <h2>Timeline</h2>
      <ul class="timeline">
        <li>
          <strong>Paper Submission Deadline</strong><br />
          <del>June 4, 2025</del>
          June 22, 2025
        </li>
        <li>
          <strong>Notification to Authors</strong><br />
          <del>June 25, 2025</del>
          July 9, 2025
        </li>
        <li>
          <strong>Camera‑Ready Deadline</strong><br />
          <del>July 11, 2025</del>
          August 14, 2025
        </li>
        <li>
          <strong>Workshop at ICCV</strong><br />
          October 19, 2025
        </li>
      </ul>
    </section>
    <section id="sponsor">
      <h2>Sponsorship</h2>
      We are seeking sponsors to help fund travel grants, the best paper award, and other workshop initiatives. If you or your organization is interested in supporting SEA, please reach out to the organizing team via email (sea_iccv2025@googlegroups.com).
    </section>
  </main>

  <!-- ================= Footer ================= -->
  <footer>
    © ICCV 2025 Sustainability with Earth Observation &amp; AI Workshop. All rights reserved.
  </footer>
  <script>
    /* random card diffusion positions */
    document.addEventListener('DOMContentLoaded', () => {
      /* Discrete‑to‑target morphing */
      const alphabet='A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z'.split('');
      function scramble(len){let s='';for(let i=0;i<len;i++){s+=alphabet[Math.floor(Math.random()*alphabet.length)];}return s;}
      document.querySelectorAll('.morph').forEach((el)=>{
        const target=el.getAttribute('data-target');
        let frame=0,maxFrame=15; // 
        const interval=setInterval(()=>{
          if(frame>=maxFrame){el.textContent=target;clearInterval(interval);return;}
          let revealCount=Math.floor(target.length*frame/maxFrame);
          let shown=target.slice(0,revealCount)+scramble(target.length-revealCount);
          el.textContent=shown;
          el.style.fontFamily='var(--font-mono)';
          frame++;
        },60);
      });
    });
  </script>
</body>
</html>
